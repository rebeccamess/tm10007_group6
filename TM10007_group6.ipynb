{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rebeccamess/tm10007_group6/blob/main/TM10007_group6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgP_eduQHJ4S",
        "outputId": "539ef1ef-41c5-4f6d-cbeb-cf53f26b38d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The number of samples: 186\n",
            "The number of features: 493\n",
            "The number of missing data: 0\n",
            "The number of zeros in the data: 4341\n",
            "The number malignent tumors are: 94\n",
            "The number benign tumors are: 92\n",
            "The number of colums with only zeros: 16\n",
            "The number of colums with only zeros in the pre_processed data: 0\n",
            "The number of zeros in the pre_processed data: 3039\n",
            "There are 3750 outliers in the dataset that are being replaced\n",
            "4.09% of the data consists of outliers\n",
            "\n",
            "------Support and Ranking for each feature------\n",
            "Passes the test:  PREDICT_original_sf_convexity_avg_2.5D  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_sf_area_avg_2.5D  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_sf_area_min_2.5D  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_logf_kurtosis_sigma1  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_logf_range_sigma1  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_logf_quartile_range_sigma5  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_logf_quartile_range_sigma10  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_mean_R3_P12  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_median_R3_P12  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_std_R3_P12  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_skewness_R3_P12  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_mean_R8_P24  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_skewness_R8_P24  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_kurtosis_R8_P24  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_quartile_range_R8_P24  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_mean_R15_P36  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_std_R15_P36  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_skewness_R15_P36  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_LBP_kurtosis_R15_P36  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_tf_GLCMMS_homogeneityd3.0A2.36mean  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_vf_Frangi_edge_quartile_range_SR(1.0, 10.0)_SS2.0  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_vf_Frangi_edge_entropy_SR(1.0, 10.0)_SS2.0  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_vf_Frangi_inner_quartile_range_SR(1.0, 10.0)_SS2.0  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_phasef_phasecong_quartile_range_WL3_N5  - Ranking:  1\n",
            "Passes the test:  PREDICT_original_phasef_phasecong_entropy_WL3_N5  - Ranking:  1\n",
            "\n",
            "------Selected Features------\n",
            "\n",
            "The accuracy on the train set is: 0.990990990990991\n",
            "The accuracy on the val set is: 0.7027027027027027\n",
            "The accuracy on the test set is: 0.7105263157894737\n"
          ]
        }
      ],
      "source": [
        "# Cloning the git respository\n",
        "!git clone https://github.com/jveenland/TM10007_ML.git\n",
        "\n",
        "!pip install boruta # use this if boruta is not installed\n",
        "# Import the needed modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "from boruta import BorutaPy\n",
        "\n",
        "# This disables all warnings future warnings that pandas gives\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# creating a random state\n",
        "np.random.seed(42)\n",
        "\n",
        "# Reading the csv and printing the number of samples and columns\n",
        "data = pd.read_csv('TM10007_ML/worcliver/Liver_radiomicFeatures.csv')\n",
        "# Summary of information about the data\n",
        "print(f'The number of samples: {len(data.index)}')\n",
        "print(f'The number of features: {len(data.columns)-2}')\n",
        "print(f'The number of missing data: {data.isnull().values.sum()}')\n",
        "print(f'The number of zeros in the data: {(data == 0).sum().sum()}')\n",
        "\n",
        "# Assess the distribution for malginant and benign in the dataset\n",
        "count_mal = (data['label'] == 'malignant').sum() \n",
        "count_ben = (data['label'] == 'benign').sum() \n",
        "print(f'The number malignent tumors are: {count_mal}')\n",
        "print(f'The number benign tumors are: {count_ben}')\n",
        "\n",
        "\n",
        "# Step 1\n",
        "# Spliting the data in a train and test set\n",
        "\n",
        "train_df, test_df = train_test_split(data, test_size = 0.2, random_state=42, stratify = data['label'])\n",
        "test_df.to_csv('test_df.csv')\n",
        "\n",
        "\n",
        "#step 2\n",
        "# calculating the the SD of a collumn\n",
        "std_colums = data.std()\n",
        "print(f'The number of colums with only zeros: {(std_colums == 0).sum().sum()}')\n",
        "# extracting all the collums that only contain zeros\n",
        "std_colums = std_colums[std_colums == False].index.tolist()\n",
        "\n",
        "# removing the collumns that only contain zeros from the data\n",
        "pre_processed_data = data.drop(columns = std_colums)\n",
        "\n",
        "# calculating the standard deviation of the collumns\n",
        "std_colums = pre_processed_data.std()\n",
        "print(f'The number of colums with only zeros in the pre_processed data: {(std_colums == 0).sum().sum()}')\n",
        "print(f'The number of zeros in the pre_processed data: {(pre_processed_data == 0).sum().sum()}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 3\n",
        "# Placing the mean in places where there is an NaN\n",
        "# Creating a list with all the means of the collumns but excluding the zeros from the caluclation\n",
        "pre_processed_data_nan = pre_processed_data.copy()\n",
        "mean = np.mean(pre_processed_data_nan)\n",
        "\n",
        "# Creating a mask with true when there is a zero\n",
        "pre_processed_data_mask = (pre_processed_data == 0)\n",
        "\n",
        "# Replace NaN values with True\n",
        "pre_processed_data = pre_processed_data.fillna(True)\n",
        "\n",
        "# Replace True values in each column with the corresponding mean value\n",
        "for col, value in mean.items():\n",
        "    pre_processed_data[col] = pre_processed_data[col].replace(True, value)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 4\n",
        "# Calculating the ouliers and replacing them with the mean of the column\n",
        "# Note that when calculating the mean the outliers are not excluded\n",
        "\n",
        "# Defining what an oulier is \n",
        "# An outlier is 2 time the standard deviation\n",
        "outlier = std_colums*2 + mean\n",
        "outlier_low = mean - std_colums*2\n",
        "\n",
        "# Making a mask that hase a True when the value is an outlier\n",
        "for col, value in mean.items(): \n",
        "  pre_processed_data_mask[col] = (pre_processed_data[col] >= outlier[col]) | (pre_processed_data[col] <= outlier_low[col])\n",
        "\n",
        "# Placing the an NaN value in the data where there is an True value in the mask\n",
        "pre_processed_data[pre_processed_data_mask] = np.nan\n",
        "\n",
        "# Replace NaN values with True\n",
        "pre_processed_data = pre_processed_data.fillna(True)\n",
        "\n",
        "# Replace True values in each column with the corresponding mean value\n",
        "for col, value in mean.items():\n",
        "    pre_processed_data[col] = pre_processed_data[col].replace(True, value)\n",
        "\n",
        "# count the number of boolean Trues\n",
        "count_true = (pre_processed_data_mask == True).sum().sum()\n",
        "\n",
        "print(f\"There are {count_true} outliers in the dataset that are being replaced\")\n",
        "percentage_outliers = (count_true/(len(data.index)*(len(data.columns)-2)))*100\n",
        "print(f\"{percentage_outliers:.2f}% of the data consists of outliers\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 5\n",
        "# scaling the features using range matching\n",
        "# select only the numeric columns\n",
        "pre_processed_data_numeric = pre_processed_data.select_dtypes(include=[float, int])\n",
        "\n",
        "# create a MinMaxScaler object\n",
        "scaler = MinMaxScaler(feature_range=(0, 100))\n",
        "\n",
        "# fit and transform the numeric DataFrame\n",
        "df_scaled = pd.DataFrame(scaler.fit_transform(pre_processed_data_numeric), columns=pre_processed_data_numeric.columns)\n",
        "\n",
        "# concatenate the scaled numeric DataFrame with the non-numeric columns\n",
        "pre_processed_data = pd.concat([pre_processed_data.select_dtypes(exclude=[float, int]), df_scaled], axis=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Step 6\n",
        "# Spliting the pre_processed_data in a train and validation set\n",
        "train_df_pre, test_df_over = train_test_split(pre_processed_data, test_size = 0.2, random_state=42, stratify = data['label'])\n",
        "train_df_pre.to_csv('train_df_pre.csv')\n",
        "\n",
        "train_df, valid_df = train_test_split(train_df_pre, test_size = 0.25, random_state=42, stratify = train_df_pre['label'])\n",
        "train_df.to_csv('train_df.csv')\n",
        "valid_df.to_csv('valid_df.csv')\n",
        "\n",
        "# creating x and y. x is the data without the label and ID, y is the data with only the label and malignent is now 1 and benign is now 0\n",
        "# train\n",
        "dropped = train_df.drop(['ID', 'label'], axis=1)\n",
        "x = train_df.drop(['ID', 'label'], axis=1)\n",
        "y = train_df['label']\n",
        "y = y.replace({'benign': 0, 'malignant': 1})\n",
        "\n",
        "# validation\n",
        "dropped_val = valid_df.drop(['ID', 'label'], axis=1)\n",
        "x_val = valid_df.drop(['ID', 'label'], axis=1)\n",
        "y_val = valid_df['label']\n",
        "y_val = y_val.replace({'benign': 0, 'malignant': 1})\n",
        "\n",
        "# test\n",
        "dropped_test = test_df_over.drop(['ID', 'label'], axis=1)\n",
        "x_test = test_df_over.drop(['ID', 'label'], axis=1)\n",
        "y_test = test_df_over['label']\n",
        "y_test = y_test.replace({'benign': 0, 'malignant': 1})\n",
        "\n",
        "\n",
        "# Step 7\n",
        "# Feature selection and model training\n",
        "# let's initialize a RF model \n",
        "model = RandomForestRegressor(n_estimators=8, random_state=42, max_depth = 5, max_features = 23, min_samples_split = 3)\n",
        "\n",
        "# let's initialize Boruta\n",
        "feat_selector = BorutaPy(\n",
        "    verbose= 0,\n",
        "    estimator=model,\n",
        "    n_estimators= 240, \n",
        "    max_iter=15,  \n",
        "    random_state=42,\n",
        "    perc = 85,\n",
        "    alpha=0.105\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# train Boruta\n",
        "# N.B.: X and y must be numpy arrays\n",
        "feat_selector.fit(np.array(x), np.array(y))\n",
        "\n",
        "# print support and ranking for each feature\n",
        "print(\"\\n------Support and Ranking for each feature------\")\n",
        "for i in range(len(feat_selector.support_)):\n",
        "    if feat_selector.support_[i]:\n",
        "        print(\"Passes the test: \", x.columns[i],\n",
        "              \" - Ranking: \", feat_selector.ranking_[i])\n",
        "        \n",
        "X_filtered = feat_selector.transform(np.array (x))\n",
        "X_filtered_val = feat_selector.transform(np.array (x_val)) \n",
        "print(\"\\n------Selected Features------\\n\")\n",
        "\n",
        "# train the model\n",
        "model.fit(X_filtered, y)\n",
        "\n",
        "# compute predictions\n",
        "predictions = model.predict(X_filtered)\n",
        "predictions_val = model.predict(X_filtered_val)\n",
        "\n",
        "# create a dataframe with real predictions and values\n",
        "df = pd.DataFrame({'pred': predictions, 'observed': y})\n",
        "df_val = pd.DataFrame({'pred': predictions_val, 'observed': y_val})\n",
        "\n",
        "\n",
        "# Step 8\n",
        "# calculating the accuracy\n",
        "true_values = df['observed'].values\n",
        "predictions = df['pred'].values\n",
        "predictions = np.where(predictions >= 0.5, 1, 0)\n",
        "\n",
        "pred_train = accuracy_score(true_values, predictions)\n",
        "print(f'The accuracy on the train set is: {pred_train}')\n",
        "\n",
        "true_values_val = df_val['observed'].values\n",
        "predictions_val = df_val['pred'].values\n",
        "predictions_val = np.where(predictions_val >= 0.5, 1, 0)\n",
        "\n",
        "pred_val = accuracy_score(true_values_val, predictions_val)\n",
        "print(f'The accuracy on the val set is: {pred_val}')\n",
        "\n",
        "# Step 9 using the test data and calculating the accuracy\n",
        "\n",
        "\n",
        "x_test_filtered = feat_selector.transform(np.array (x_test)) \n",
        "predictions_test = model.predict(x_test_filtered)\n",
        "df_test = pd.DataFrame({'pred': predictions_test, 'observed': y_test})\n",
        "\n",
        "###\n",
        "true_values_test = df_test['observed'].values\n",
        "predictions_test = df_test['pred'].values\n",
        "predictions_test = np.where(predictions_test >= 0.5, 1, 0)\n",
        "\n",
        "pred_test = accuracy_score(true_values_test, predictions_test)\n",
        "print(f'The accuracy on the test set is: {pred_test}')"
      ]
    }
  ]
}